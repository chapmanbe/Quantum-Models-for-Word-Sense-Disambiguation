{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check limit classically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "fiel_new_vectors = \"new_noun_vectors.json\"\n",
    "with open(fiel_new_vectors, 'r') as fp:\n",
    "    new_noun_vectors = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"report/figure/results/\"):\n",
    "    os.makedirs(\"report/figure/results/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with file, register, smooth\n",
    "import numpy as np\n",
    "from words import sentences as sentences\n",
    "\n",
    "\n",
    "triplets = [\n",
    "    [\"register\", \"smooth\"],\n",
    "    [\"drip\", \"carry\"],\n",
    "    [\"knock\", \"intercept\"],\n",
    "    [\"bill\", \"accuse\"]\n",
    "]\n",
    "\n",
    "verb_vectors = {}\n",
    "true_sentences = {}\n",
    "for triplet in triplets:\n",
    "\n",
    "    \n",
    "    nouns, verbs = set(), set()\n",
    "    for sentence in sentences:\n",
    "        _s = sentence.split(\" \")\n",
    "        if _s[0] in triplet:\n",
    "            true_sentences[(_s[0],_s[1])] = int(_s[2][1])\n",
    "            verbs.add(_s[0])\n",
    "            nouns.add(_s[1])\n",
    "\n",
    "    n_verbs = len(verbs)\n",
    "    noun_list = list(nouns)\n",
    "    verb_list = list(verbs)\n",
    "    all_combinations = [(verb, noun) for verb in verb_list for noun in noun_list]\n",
    "    plausabilities = [true_sentences.get(sent,0) for sent in all_combinations]\n",
    "    S = np.array(plausabilities).reshape((n_verbs,len(nouns)))\n",
    "    N = np.array([new_noun_vectors[noun] for noun in noun_list]).transpose()\n",
    "\n",
    "    N_inv = np.linalg.pinv(N)\n",
    "    V = np.dot(S,N_inv)\n",
    "    v_calc = {verb: V[i]/np.linalg.norm(V[i]) for i,verb in enumerate(verbs)}\n",
    "    verb_vectors = {**verb_vectors, **v_calc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "for sentence in sentences:\n",
    "    _s = sentence.split(\" \")\n",
    "    true_result = int(_s[2][1])\n",
    "    try:\n",
    "        calc_result = np.abs(np.dot(new_noun_vectors[_s[1]],verb_vectors[_s[0]]))\n",
    "        loss.append((true_result-calc_result)**2)\n",
    "    except:\n",
    "        pass\n",
    "mse = np.mean(loss)\n",
    "print(\"Baseline: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"experiments/density_matrix_model\"\n",
    "evo_path = os.path.join(folder,\"evo.json\")\n",
    "with open(evo_path) as json_file:\n",
    "    evo = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dims = (10,7)\n",
    "fig, ax = plt.subplots(figsize=dims)\n",
    "plt.rcParams[\"font.size\"] = \"14\"\n",
    "plt.plot(range(len(evo)), evo, '-b', label='loss')\n",
    "plt.plot(range(len(evo)), [mse]*len(evo), '--r', label='baseline')\n",
    "plt.ylim([0,0.3])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend(loc='upper right')\n",
    "#plt.title(\"temp\")\n",
    "plt.savefig(\"./report/figure/results/spsa_density_matrix_fit.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate density matrices from quantum fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best fit\n",
    "with open(os.path.join(folder,\"params.json\"), 'r') as fp:\n",
    "    quantum_params = json.load(fp)\n",
    "print(quantum_params[\"register\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the quantum ansÃ¤tze\n",
    "from discopy.quantum import CX, Circuit, CRz, H, Ket, Rx, Rz, Ry, X, sqrt, C, SWAP, CRy, Bra\n",
    "n_verb_params = 6\n",
    "\n",
    "\n",
    "def verb_ansatz(p):\n",
    "    return Ket(0,0) >> \\\n",
    "        Rx(p[0]) @ Rx(p[1]) >> \\\n",
    "        Ry(p[2]) @ Ry(p[3]) >> \\\n",
    "        Rz(p[4]) @ Rz(p[5]) >> \\\n",
    "        CX >> SWAP >> CX >> SWAP \n",
    "        \n",
    "def noun_ansatz(arr):\n",
    "    a1 = np.linalg.norm(arr[0:2])\n",
    "    a2 = np.linalg.norm(arr[2:])\n",
    "    phi1 = np.arccos(a1)/np.pi\n",
    "\n",
    "    # fix issues with rotations\n",
    "    rot1 = arr[0:2]/a1\n",
    "    phi2_cos = np.arccos(rot1[0])/np.pi\n",
    "    phi2_sin = np.arcsin(rot1[1])/np.pi\n",
    "    if not np.sign(phi2_cos) == np.sign(phi2_sin):\n",
    "        phi2_cos *= -1\n",
    "    rot2 = arr[2: ]/a2\n",
    "    phi3_cos = np.arccos(rot2[0])/np.pi\n",
    "    phi3_sin = np.arcsin(rot2[1])/np.pi\n",
    "    if not np.sign(phi3_cos) == np.sign(phi3_sin):\n",
    "        phi3_cos *= -1\n",
    "\n",
    "    return Ket(0,0) >> Ry(phi1) @ Circuit.id(1) >> CRy(phi3_cos) >> X @ Circuit.id(1) >> CRy(phi2_cos) >> X @ Circuit.id(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start with \"file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## quantum model\n",
    "verb_states = {verb: verb_ansatz(quantum_params[verb][\"p\"]).eval().array.flatten() for verb in verb_vectors}\n",
    "noun_states = {noun: noun_ansatz(quantum_params[noun][\"p\"]).eval().array.flatten() for noun in new_noun_vectors}\n",
    "\n",
    "# purely classical model\n",
    "#verb_states = verb_vectors\n",
    "#noun_states = new_noun_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classically calculated loss\n",
    "loss = []\n",
    "k=0\n",
    "for sentence in sentences:\n",
    "    _s = sentence.split(\" \")\n",
    "    true_result = int(_s[2][1])\n",
    "    try:\n",
    "        calc_result = np.abs(np.dot(noun_states[_s[1]],verb_states[_s[0]]))\n",
    "        #print(sentence)\n",
    "        #print(calc_result)\n",
    "        loss.append((true_result-calc_result)**2)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "mse = np.mean(loss)\n",
    "#print(loss)\n",
    "print(\"Classically calc MSE: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb = verb_states[\"smooth\"]\n",
    "noun = noun_states[\"tooth\"]\n",
    "np.abs(np.dot(verb,noun))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_file = 1/2 * np.outer(np.conj(verb_states[\"register\"]),verb_states[\"register\"]) + \\\n",
    "    1/2 * np.outer(np.conj(verb_states[\"smooth\"]),verb_states[\"smooth\"])\n",
    "print(rho_file.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import logm\n",
    "def von_neumann_entropy(rho):\n",
    "    return np.abs(np.trace(np.dot(rho, logm(rho))))\n",
    "\n",
    "von_neumann_entropy(rho_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_account = np.outer(np.conj(noun_states[\"account\"]),noun_states[\"account\"])\n",
    "von_neumann_entropy(rho_account).round(5)\n",
    "von_neumann_entropy(np.multiply(rho_account,rho_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from words import noun_groups, pairs\n",
    "from pprint import pprint\n",
    "\n",
    "s_dict = {}\n",
    "disamb = []\n",
    "disamb_rand = []\n",
    "for amb_verb in pairs:\n",
    "    sense1, sense2 = pairs[amb_verb]\n",
    "    rho = 1/2 * np.outer(verb_states[sense1],np.conj(verb_states[sense1])) + \\\n",
    "        1/2 * np.outer(verb_states[sense2],np.conj(verb_states[sense2]))\n",
    "    s = von_neumann_entropy(rho)\n",
    "    s_dict[amb_verb] = {\"init\": s, \"disamb\": {}, \"rand\": []}\n",
    "\n",
    "    for noun in noun_groups[amb_verb]:\n",
    "        rho_n = np.outer(noun_states[noun],np.conj(noun_states[noun]))\n",
    "        rho_comp = np.multiply(rho_n,rho)\n",
    "        rho_comp /= np.trace(rho_comp)\n",
    "        s_comp = von_neumann_entropy(rho_comp)\n",
    "        s_dict[amb_verb][\"disamb\"][noun] = s_comp\n",
    "        disamb.append((s-s_comp)/s)\n",
    "\n",
    "        i=0\n",
    "        while i<10:\n",
    "            # per word get some random vectors for disambiguation\n",
    "            z = np.random.uniform(-1,1,size=(4,))#.view(np.complex128)\n",
    "            z /= np.linalg.norm(z)\n",
    "\n",
    "            l = []\n",
    "            for noun in noun_groups[amb_verb]:\n",
    "                l.append(np.abs(np.dot(z,noun_states[noun])))\n",
    "            if any(i >= 0.7 for i in l):\n",
    "                continue\n",
    "            i+=1\n",
    "            rho_rand = np.outer(z,np.conj(z))\n",
    "            rho_comp_rand = np.multiply(rho_rand,rho)\n",
    "            rho_comp_rand /= np.trace(rho_comp_rand)\n",
    "            s_comp_rand = von_neumann_entropy(rho_comp_rand)\n",
    "            s_dict[amb_verb][\"rand\"].append(von_neumann_entropy(rho_comp_rand))\n",
    "            disamb_rand.append((s-s_comp_rand)/s)\n",
    "\n",
    "pprint(s_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "# create boxplots\n",
    "n_list = [\"context\" for i in range(len(disamb))]\n",
    "r_list = [\"random\" for i in range(len(disamb_rand))]\n",
    "data = pd.DataFrame({\"Disambigutation Power\": disamb+disamb_rand, \"type\": n_list+r_list})\n",
    "\n",
    "dims = (10,7)\n",
    "fig, ax = plt.subplots(figsize=dims)\n",
    "b = sns.boxplot(ax=ax, x=\"type\", y=\"Disambigutation Power\", data=data)\n",
    "b.tick_params(labelsize=14)\n",
    "b.set_xlabel(\"Type\",fontsize=16)\n",
    "b.set_ylabel(\"Disambigutation Power\",fontsize=16)\n",
    "ax.set(ylim=(-0.2, 1));  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(disamb))\n",
    "print(np.median(disamb_rand))\n",
    "print(len(n_list))\n",
    "print(len(r_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums\n",
    "\n",
    "ranksums(disamb,disamb_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity measure of disambiguated verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from words import noun_groups, pairs\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "save = {\"word\": [], \"similarity\": [], \"score\": []}\n",
    "results = {amb_verb: [] for amb_verb in pairs}\n",
    "for amb_verb in pairs:\n",
    "    # construct density matrix of ambigious verb\n",
    "    sense1, sense2 = pairs[amb_verb]\n",
    "    rho = 1/2 * np.outer(verb_states[sense1],np.conj(verb_states[sense1])) + \\\n",
    "        1/2 * np.outer(verb_states[sense2],np.conj(verb_states[sense2]))\n",
    "\n",
    "    for noun in noun_groups[amb_verb]:\n",
    "        rho_n = np.outer(noun_states[noun],np.conj(noun_states[noun]))\n",
    "        rho_comp = np.multiply(rho_n,rho)\n",
    "        rho_comp /= np.trace(rho_comp)\n",
    "\n",
    "        # calculate similarity to random word\n",
    "        z = np.random.uniform(-1,1,size=(4,))\n",
    "        z /= np.linalg.norm(z)\n",
    "        rho_rand = np.matrix(np.outer(z,np.conj(z)))\n",
    "        rho_comp_rand = np.multiply(rho_rand,rho)\n",
    "        rho_comp_rand /= np.trace(rho_comp_rand)\n",
    "\n",
    "        max_rand_sim = 0\n",
    "        for sense in pairs[amb_verb]:\n",
    "\n",
    "            rho_sense = np.matrix(np.outer(verb_states[sense],np.conj(verb_states[sense])))\n",
    "            similarity = np.abs(np.trace(np.dot(rho_sense.getH(),rho_comp)))\n",
    "            gtruth = true_sentences.get((sense,noun),0)\n",
    "            \n",
    "            save[\"word\"].append(amb_verb)\n",
    "\n",
    "            if gtruth == 0:\n",
    "                save[\"similarity\"].append(\"context\")\n",
    "                save[\"score\"].append(1-similarity)\n",
    "                results[amb_verb].append([gtruth, similarity])\n",
    "            else:\n",
    "                save[\"similarity\"].append(\"context\")\n",
    "                save[\"score\"].append(similarity)\n",
    "                results[amb_verb].append([gtruth, similarity])\n",
    "\n",
    "            similarity_rand = np.abs(np.trace(np.dot(rho_sense.getH(),rho_comp_rand)))\n",
    "            max_rand_sim = max(max_rand_sim,similarity_rand)\n",
    "\n",
    "        save[\"word\"].append(amb_verb)\n",
    "        save[\"similarity\"].append(\"random\")\n",
    "        save[\"score\"].append(max_rand_sim)\n",
    "            \n",
    "save_df = pd.DataFrame.from_dict(save)\n",
    "save_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set(font_scale=1.5)\n",
    "dims = (10,7)\n",
    "fig, ax = plt.subplots(figsize=dims)\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "sns.boxplot(ax=ax, x=\"word\", y=\"score\", hue=\"similarity\",\n",
    "                 data=save_df, palette=\"Set3\")\n",
    "plt.legend(prop={\"size\":15});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for difference in distribution\n",
    "for amb_verb in results:\n",
    "    idx1 = (save_df.word == amb_verb) & (save_df.similarity==\"context\")\n",
    "    idx2 = (save_df.word == amb_verb) & (save_df.similarity==\"random\")\n",
    "    print(amb_verb,ranksums(save_df[idx1].score,save_df[idx2].score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qnlp-ws)",
   "language": "python",
   "name": "qnlp-ws"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "metadata": {
   "interpreter": {
    "hash": "597cb9212bb6505db551d0fa5a39e44e38662ecd79044286d7f1c9c46fe55c66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
